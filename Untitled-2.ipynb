{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3.1: An example to demonstrate loading and visualisation of the iris dataset with sklearn dataset library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0                5.1               3.5                1.4               0.2\n",
      "1                4.9               3.0                1.4               0.2\n",
      "2                4.7               3.2                1.3               0.2\n",
      "3                4.6               3.1                1.5               0.2\n",
      "4                5.0               3.6                1.4               0.2\n",
      "5                5.4               3.9                1.7               0.4\n",
      "6                4.6               3.4                1.4               0.3\n",
      "7                5.0               3.4                1.5               0.2\n",
      "8                4.4               2.9                1.4               0.2\n",
      "9                4.9               3.1                1.5               0.1\n",
      "   irisType\n",
      "0         0\n",
      "1         0\n",
      "2         0\n",
      "3         0\n",
      "4         0\n",
      "5         0\n",
      "6         0\n",
      "7         0\n",
      "8         0\n",
      "9         0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import the required \n",
    "\n",
    "import sklearn as sklearn\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Load the iris data to memory\n",
    "iris = load_iris()\n",
    "\n",
    "# The loaded iris is a dictionary - so check it\n",
    "iris.keys()\n",
    "\n",
    "# You can print the description of the dataset\n",
    "print(iris.DESCR)\n",
    "\n",
    "#Convert the loaded data and targets into a dataframe and print the first ten rows.\n",
    "X = pd.DataFrame(data = iris.data, columns = iris.feature_names)\n",
    "\n",
    "#print(X.head())\n",
    "print(X.head(10))\n",
    "Y = pd.DataFrame(data=iris.target, columns = ['irisType'])\n",
    "print(Y.head(10))\n",
    "\n",
    "#Explore the number of classes in the target set\n",
    "Y.irisType.value_counts()\n",
    "\n",
    "#The names of the classes(i.e. species in the iris dataset)\n",
    "iris.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3.2: Example to illustrate the use of MAE, MSE and Cross Entropy with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mean Absolute Error is -  2.0\n",
      "\n",
      "The Mean Square Error is -  5.0\n",
      "\n",
      "The Categorical Cross Entropy Loss is -  0.2876821\n",
      "\n",
      "The Sparse Categorical Cross Entropy Loss is -  0.28768212\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras  # Updated import for Keras\n",
    "\n",
    "# Losses are now part of the keras.losses submodule\n",
    "from tensorflow.keras.losses import MeanAbsoluteError\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "## Define y_true and y_pred for a regression task\n",
    "y_true = np.array([1., 0.])  # Use tf.constant for tensors\n",
    "y_pred = np.array([2., 3.])\n",
    "\n",
    "# Compute and Print the MAE\n",
    "mae_loss = MeanAbsoluteError()\n",
    "print(\"The Mean Absolute Error is - \", mae_loss(y_true, y_pred).numpy())\n",
    "\n",
    "# Compute and Print the MSE\n",
    "mse_loss = MeanSquaredError()\n",
    "print(\"\")\n",
    "print(\"The Mean Square Error is - \", mse_loss(y_true, y_pred).numpy())\n",
    "\n",
    "## Define y_true and y_pred for a classification task\n",
    "# Use one-hot vector representation\n",
    "y_true = tf.constant([[0, 1, 0], [1, 0, 0]])\n",
    "y_pred = tf.constant([[0.15, 0.75, 0.1], [0.75, 0.15, 0.1]])\n",
    "\n",
    "# Compute and print the categorical-cross-entropy-loss\n",
    "cross_entropy_loss = CategoricalCrossentropy()\n",
    "print(\"\")\n",
    "print(\"The Categorical Cross Entropy Loss is - \", cross_entropy_loss(y_true, y_pred).numpy())\n",
    "\n",
    "# Use label-encoded representation for the class\n",
    "y_true = tf.constant([1, 0])\n",
    "y_pred = tf.constant([[0.15, 0.75, 0.1], [0.75, 0.15, 0.1]])\n",
    "\n",
    "# Compute and print the sparse-cross-entropy-loss\n",
    "cross_entropy_loss = SparseCategoricalCrossentropy()\n",
    "print(\"\")\n",
    "print(\"The Sparse Categorical Cross Entropy Loss is - \", cross_entropy_loss(y_true, y_pred).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted v1: 1.6\n",
      "Predicted v2: 6.0\n",
      "Mean Squared Error for v1: 0.0\n",
      "Mean Squared Error for v2: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming vt, r1, and r2 are given\n",
    "vt = 10\n",
    "r1 = 2\n",
    "r2 = 3\n",
    "\n",
    "# Calculate v1 and v2\n",
    "v1 = (vt - r1) / (r1 + r2)\n",
    "v2 = (vt * r2) / (r1 + r2)\n",
    "\n",
    "# Prepare the data for linear regression\n",
    "X = [[r1, r2]]\n",
    "y_v1 = [v1]\n",
    "y_v2 = [v2]\n",
    "\n",
    "# Train the linear regression models\n",
    "reg_v1 = LinearRegression().fit(X, y_v1)\n",
    "reg_v2 = LinearRegression().fit(X, y_v2)\n",
    "\n",
    "# Predict v1 and v2 for given vt, r1, and r2\n",
    "predicted_v1 = reg_v1.predict([[r1, r2]])\n",
    "predicted_v2 = reg_v2.predict([[r1, r2]])\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse_v1 = mean_squared_error(y_v1, predicted_v1)\n",
    "mse_v2 = mean_squared_error(y_v2, predicted_v2)\n",
    "\n",
    "print(\"Predicted v1:\", predicted_v1[0])\n",
    "print(\"Predicted v2:\", predicted_v2[0])\n",
    "print(\"Mean Squared Error for v1:\", mse_v1)\n",
    "print(\"Mean Squared Error for v2:\", mse_v2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax: [0.22734376 0.1684204  0.14496078 0.15239308 0.30688198]\n",
      "ReLU: [1.2  0.9  0.75 0.8  1.5 ]\n",
      "Bipolar Sigmoid: [0.53704957 0.42189901 0.3583574  0.37994896 0.63514895]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Weighted sum of five neurons\n",
    "weighted_sum = np.array([1.2, 0.9, 0.75, 0.8, 1.5])\n",
    "\n",
    "# Softmax\n",
    "exp = np.exp(weighted_sum)\n",
    "softmax = exp / np.sum(exp)\n",
    "print(\"Softmax:\", softmax)\n",
    "\n",
    "# ReLU\n",
    "relu = np.maximum(0, weighted_sum)\n",
    "print(\"ReLU:\", relu)\n",
    "\n",
    "# Bipolar sigmoid with ðœŽ = 1\n",
    "sigmoid = (1 - np.exp(-weighted_sum)) / (1 + np.exp(-weighted_sum))\n",
    "print(\"Bipolar Sigmoid:\", sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output using Softmax: [0.22734376 0.1684204  0.14496078 0.15239308 0.30688198]\n",
      "Output using ReLU: [1.2  0.9  0.75 0.8  1.5 ]\n",
      "Output using Bipolar Sigmoid: [0.53704957 0.42189901 0.3583574  0.37994896 0.63514895]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Weighted sum of neurons\n",
    "weighted_sum = np.array([1.2, 0.9, 0.75, 0.8, 1.5])\n",
    "\n",
    "# Softmax activation function\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "# ReLU activation function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Bipolar sigmoid activation function with sigma = 1\n",
    "def bipolar_sigmoid(x):\n",
    "    return 2 / (1 + np.exp(-x)) - 1\n",
    "\n",
    "# Compute outputs using the activation functions\n",
    "output_softmax = softmax(weighted_sum)\n",
    "output_relu = relu(weighted_sum)\n",
    "output_bipolar_sigmoid = bipolar_sigmoid(weighted_sum)\n",
    "\n",
    "print(\"Output using Softmax:\", output_softmax)\n",
    "print(\"Output using ReLU:\", output_relu)\n",
    "print(\"Output using Bipolar Sigmoid:\", output_bipolar_sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 4.2\n",
    "## Develop a Python Code with Sklearn framework for classification of the first two species of the Iris Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Perceptron Confusion Matrix:\n",
      " [[12  0]\n",
      " [ 0  8]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#Import required libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "#Load the Iris dataset\n",
    "iris = load_iris()\n",
    "#View the iris data keys\n",
    "iris.keys()#it is a key of dictionary iris\n",
    "iris.target_names\n",
    "##Convert data and target into a data frame.\n",
    "#Extract the First 100 Features\n",
    "X = pd.DataFrame(data = iris.data[:100,:], columns = iris.feature_names)\n",
    "X.head(100) #Inspect the features\n",
    "#Target\n",
    "y = pd.DataFrame(data=iris.target[:100], columns = ['irisType'])\n",
    "y.head(100)\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)# Create a Perceptron classifier\n",
    "mdlPercept = Perceptron()\n",
    "# Train the classifier\n",
    "mdlPercept.fit(X_train, y_train)\n",
    "# Make predictions on the test set\n",
    "y_pred = mdlPercept.predict(X_test)\n",
    "# Calculate accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "#Generate the Confusion Matrix\n",
    "print(\"Perceptron Confusion Matrix:\\n\",confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9385964912280702\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.84      0.91        43\n",
      "           1       0.91      1.00      0.95        71\n",
      "\n",
      "    accuracy                           0.94       114\n",
      "   macro avg       0.96      0.92      0.93       114\n",
      "weighted avg       0.94      0.94      0.94       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the MLP Classifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
